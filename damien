#!/usr/bin/env python
__globals = globals().copy()
import argparse, json, hashlib, pymongo, bson, tabulate, time, gridfs, argcomplete, os

main_parser = argparse.ArgumentParser()
main_parser.add_argument('--dbname', type=str, nargs='?', default='toto')
main_subparsers = main_parser.add_subparsers()

collector_parser = main_subparsers.add_parser('collector')
def collector(db, args):
    import docker, threading
    def collect(db, clt, Id):
        try:
            db.dockercontainers.insert_one(clt.inspect_container(Id))
        except Exception as e:
            print(e)
        count = 0
        for stat in clt.stats(Id, decode=True):
            count += 1
            stat['Id'] = Id
            db.dockerstats.insert_one(stat)
            print(Id, count)
    def spawn_worker(Id):
            t = threading.Thread(target=collect, args=(db, clt, Id))
            t.daemon = True
            t.start()
    db.dockercontainers.create_index([('Id', pymongo.ASCENDING)], unique=True)
    db.dockerstats.create_index([('read', pymongo.ASCENDING), ('Id', pymongo.ASCENDING)], unique=True)
    clt = docker.Client()
    for container in clt.containers():
        spawn_worker(container['Id'])
    for event in clt.events(decode=True):
        print(event)
        if 'status' not in event:
            continue
        status = event['status']
        if status == 'start':
            spawn_worker(event['id'])
    pass
collector_parser.set_defaults(func=collector)

httpd_parser = main_subparsers.add_parser('httpd')
def httpd(db, args):
    import lib.HTML as HTML
    import bottle, csv, StringIO, datetime, itertools
    import pandas as pd
    import numpy as np
    import plotly.offline
    import plotly.graph_objs
    fs = gridfs.GridFS(db)
    @bottle.route('/run')
    def run_list():
        table = [['runId', 'configId', 'config']]
        for run in db.run.find({'status':'done'}):
            runId = run['runId']
            configId = run['configId']
            link_runId = HTML.link(runId[-4:],'/run/%s' % runId)
            link_configId = HTML.link(configId[:4],'/config/%s' % configId)
            config = next(db.config.find({'configId':configId},{'_id':0, 'sourceId':0, 'configId':0}))
            table.append([link_runId, link_configId, str(config)])
        return HTML.table(table)
    @bottle.route('/run/<runId>')
    def run_show(runId):
        table = []
        run = next(db.run.find({'runId':runId}))
        if 'container' in run:
            container = run['container']
            row = ['container', HTML.link('inspect', '/dockercontainers/%s' % container['Id'])]
            for d in db.dockerstats.find({'Id':container['Id']}).limit(1):
                row += [HTML.link("%s.%s" % (stat, ext),
                        '/dockerstats/%s/%s.%s' % (container['Id'], stat, ext) )
                        for ext in ['csv', 'html']
                        for stat in ['cpu','memory','blkio', 'netio']]
            for d in db.sysbench.find({'Id':container['Id']}).limit(1):
                row += [HTML.link('sysbench.%s' % ext, '/sysbench/%s/perf.%s' % (container['Id'], ext)) for ext in ['csv', 'html']]
            table.append(row)
        return HTML.table(table)
    @bottle.route('/dockercontainers/<Id>')
    def dockercontainers(Id):
        for container in db.dockercontainers.find({'Id':Id}).limit(1):
            return str(container)
        return "Not Found"
    @bottle.route('/dockerstats/<listId>/<stat>.csv')
    def dockerstats_csv(listId,stat):
        if stat not in ['memory', 'blkio']:
            return "Oops"
        directory = os.path.join('cache','dockerstats','listId')
        if not os.path.exists(directory):
            os.makedirs(directory)
        filename = os.path.join(directory, stat + '.csv')
        # cache miss
        if not os.path.exists(filename):
            with open(filename, 'w') as f:
                csvwriter = csv.writer(f)
                csvwriter.writerow(['x','y','label'])
                for Id in listId.split(','):
                    label = [Id[:4]]
                    if stat == 'memory':
                        for entry in db.dockerstats.find({'Id':Id},{'memory_stats':1,'read':1}):
                            x = entry['read']
                            x = datetime.datetime.strptime(x[:-4], "%Y-%m-%dT%H:%M:%S.%f")
                            y = entry['memory_stats']
                            def flat(key, y):
                                if type(y) == dict:
                                    for k, v in y.iteritems():
                                        flat(key+[k], v)
                                elif type(y) == list:
                                    print(y)
                                else:
                                    csvwriter.writerow([x,y,".".join(key)])
                            flat(label,y)
                    elif stat == 'blkio':
                        X = []
                        YREAD = []
                        YWRITE = []
                        for entry in db.dockerstats.find({'Id':Id},{'blkio_stats':1,'read':1}):
                            x = entry['read']
                            x = datetime.datetime.strptime(x[:-4], "%Y-%m-%dT%H:%M:%S.%f")
                            yr = 0
                            yw = 0
                            for data in entry['blkio_stats']['io_service_bytes_recursive']:
                                if data['op'] == 'Write':
                                    yw += data['value']
                                elif data['op'] == 'Read':
                                    yr += data['value']
                            X.append(x)
                            YWRITE.append(yw)
                            YREAD.append(yr)
                        dx = np.gradient([time.mktime(x.timetuple()) for x in X])
                        YWRITE = np.gradient(YWRITE, dx)
                        YREAD = np.gradient(YREAD, dx)
                        for x,y in itertools.izip(X,YWRITE):
                            csvwriter.writerow([x,y,".".join(label+['w'])])
                        for x,y in itertools.izip(X,YREAD):
                            csvwriter.writerow([x,y,".".join(label+['r'])])
        with open(filename) as f:
            return f.read()
        return "x,y,label\n0,0,oops\n1,1,oops\n"
    @bottle.route('/sysbench/<listId>/perf.csv')
    def sysbench_csv(listId):
        return "x,y,label\n0,0,todo\n1,1,todo\n"
    @bottle.route('/<collection>/<selector>/<path>.html')
    def plot_any(collection, selector, path):
        environ = {'REQUEST_METHOD' : 'GET', 'PATH_INFO' : '/%s/%s/%s.csv' % (collection, selector, path)}
        target, args = bottle.default_app().router.match(environ)
        csv = target(**args)
        df = pd.read_csv(StringIO.StringIO(csv))
        def label_sel_generator(df):
            for label in np.unique(df['label']):
                sel = df['label'] == label
                yield label, sel
        def X_Y_label_generator(df):
            for label, sel in label_sel_generator(df):
                X = df['x'][sel]
                Y = df['y'][sel]
                yield X, Y, label
        data = [plotly.graph_objs.Scatter(x=X, y=Y, name=label, visible="legendonly") for (X,Y,label) in X_Y_label_generator(df)]
        layout = plotly.graph_objs.Layout(showlegend=True)
        figure = plotly.graph_objs.Figure(data=data, layout=layout)
        plotly.offline.plot(figure, filename='toto.html', auto_open=False)
        with open('toto.html') as f:
            return f.read()
        return "Oops"
    @bottle.route('/config')
    def config_list():
        table = [['configId', 'sourceId', 'values']]
        for config in db.config.find({},{'_id':0}):
            configId = config['configId']
            del config['configId']
            sourceId = config['sourceId']
            del config['sourceId']
            values = str(config)
            link_configId = HTML.link(configId[:4], '/config/%s' % configId)
            link_sourceId = HTML.link(sourceId[:4], '/source/%s' % sourceId)
            table.append([link_configId, link_sourceId, str(config)])
        return HTML.table(table)
    @bottle.route('/config/<configId>')
    def config_show(configId):
        table = [['runId', 'status', 'extra']]
        for run in db.run.find({'configId':configId}, {'_id':0, 'configId':0}):
            runId = run['runId']
            del run['runId']
            status = run['status']
            del run['status']
            extra = str(run)
            link_runId = HTML.link(runId[-4:], '/run/%s' % runId)
            table.append([link_runId, status, extra])
        return HTML.table(table)
    @bottle.route('/source/<sourceId>')
    def source_show(sourceId):
        source = matches_only_one_source(fs, sourceId)
        return source.read()[:-1]
    bottle.run(host='0.0.0.0', port=8080, debug=True)
httpd_parser.set_defaults(func=httpd)

daemon_parser = main_subparsers.add_parser('daemon')
def daemon(db, args):
    fs = gridfs.GridFS(db)
    while True:
        cursor = db.run.find({'status' : 'created'}).sort([('runId',pymongo.ASCENDING)]).limit(1)
        try:
            run = next(cursor)
            runId = run['runId']
            config = matches_only_one_config(db, run['configId'])
            source = matches_only_one_source(fs, config['sourceId'])
            source = source.read()[:-1]
            print('Running %s' % run['runId'])
            _globals = __globals.copy()
            _globals['config'] = config
            _globals['run'] = run
            _globals['db'] = db
            _locals = _globals
            exec(source,_globals,_locals)
            db.run.update_one({'_id':run['_id']}, {"$set": {'status':'done'}})
        except StopIteration:
            print('Waiting for a new run')
            time.sleep(1)
daemon_parser.set_defaults(func=daemon)

source_parser = main_subparsers.add_parser('source')
source_subparsers = source_parser.add_subparsers()

def source_add(db, args):
    with open(args.py_file) as py_file:
        fs = gridfs.GridFS(db)
        source = py_file.read()
        sourceId = hashlib.sha224(source).hexdigest()
        if not fs.exists({'type':'source', 'sourceId':sourceId}):
            fs.put(source, type='source', sourceId=sourceId, filename=args.py_file)
        print(sourceId)
source_add_parser = source_subparsers.add_parser('add')
source_add_parser.set_defaults(func=source_add)
source_add_parser.add_argument('py_file', type=str)

def source_list(db, args):
    fs = gridfs.GridFS(db)
    cursor = fs.find({'type':'source'})
    table = [[source.name, source.sourceId] for source in cursor]
    print(tabulate.tabulate(table, headers=['name', 'sourceId'], tablefmt='plain'))
source_list_parser = source_subparsers.add_parser('list')
source_list_parser.set_defaults(func=source_list)

def matches_only_one_file(fs, type, field, regex):
    cursor = fs.find({'type' : type, field : {'$regex':regex}})
    count = 0
    for f in cursor:
        if count == 1:
            raise Exception('More than one match!')
        count += 1
    if count == 0:
        raise Exception('No match')
    return f

def matches_only_one_source(fs, regex):
    return matches_only_one_file(fs, 'source', 'sourceId', regex)

def source_show(db, args):
    fs = gridfs.GridFS(db)
    f = matches_only_one_source(fs, args.sourceId)
    print(f.read()[:-1])
source_show_parser = source_subparsers.add_parser('show')
source_show_parser.set_defaults(func=source_show)
source_show_parser.add_argument('sourceId')

def source_delete(db, args):
    fs = gridfs.GridFS(db)
    f = matches_only_one_source(fs, args.sourceId)
    sourceId = f.sourceId
    # delete config ref on source first!
    for config in db.config.find({'sourceId':sourceId}).limit(1):
        raise Exception('Found config referencing this sourceId=%s : configId=%s' % (sourceId, config['configId']))
    fs.delete(f._id)
    print(sourceId)
source_delete_parser = source_subparsers.add_parser('delete')
source_delete_parser.set_defaults(func=source_delete)
source_delete_parser.add_argument('sourceId')

config_parser = main_subparsers.add_parser('config')
config_subparsers = config_parser.add_subparsers()

def config_add(db, args):
    with open(args.json_file) as json_file:
        fs = gridfs.GridFS(db)
        config = json.load(json_file)
        if 'configId' in config:
            del config['configId']
        configId = hashlib.sha224(json.dumps(config,sort_keys=True)).hexdigest()
        config['configId'] = configId
        sourceId = config['sourceId']
        matches_only_one_source(fs, sourceId)
        collection = db.config
        collection.create_index('configId', unique=True, sparse=True)
        collection.insert_one(config)
        print(configId)
config_add_parser = config_subparsers.add_parser('add')
config_add_parser.set_defaults(func=config_add)
config_add_parser.add_argument('json_file', type=str)

def config_list(db, args):
    collection = db.config
    cursor = collection.find()
    table = [[config['configId']] for config in cursor]
    print(tabulate.tabulate(table, headers=['configId'], tablefmt='plain'))
config_list_parser = config_subparsers.add_parser('list')
config_list_parser.set_defaults(func=config_list)

def matches_only_one(collection, field, regex):
    cursor = collection.find({field: {'$regex':regex}})
    count = 0
    for doc in cursor:
        if count == 1:
            raise Exception('More than one match!')
        count += 1
    if count == 0:
        raise Exception('No match')
    return doc

def matches_only_one_config(db, regex):
    return matches_only_one(db.config, 'configId', regex)

def config_show(db, args):
    config = matches_only_one_config(db, args.configId)
    del config['_id']
    print(json.dumps(config, sort_keys=True, indent=1))
config_show_parser = config_subparsers.add_parser('show')
config_show_parser.set_defaults(func=config_show)
config_show_parser.add_argument('configId', type=str)

def config_delete(db, args):
    config = matches_only_one_config(db, args.configId)
    configId = config['configId']
    # should rm run ref on config first!
    for run in db.run.find({'configId':configId}).limit(1):
        raise Exception('Found run referencing this configId=%s : runId=%s' % (configId, run['runId']))
    result = db.config.delete_one({'configId' : configId})
    print(configId)
config_delete_parser = config_subparsers.add_parser('delete')
config_delete_parser.set_defaults(func=config_delete)
config_delete_parser.add_argument('configId', type=str)

run_parser = main_subparsers.add_parser('run')
run_subparsers = run_parser.add_subparsers()

def run_list(db, args):
    collection = db.run
    cursor = collection.find()
    table = [[run['runId'], run['status'], run['configId']] for run in cursor]
    print(tabulate.tabulate(table, headers=['runId', 'status', 'configId'], tablefmt="plain"))
run_list_parser = run_subparsers.add_parser('list')
run_list_parser.set_defaults(func=run_list)

def run_new(db, args):
    config = matches_only_one_config(db, args.configId)
    run = { 'status' : 'created', 'configId' : config['configId'] }
    runId = db.run.insert_one(run).inserted_id
    db.run.update_one({'_id' : runId}, {"$set" : { 'runId' : str(runId) }})
    db.run.create_index('runId', unique=True, sparse=True)
    print(runId)
run_new_parser = run_subparsers.add_parser('new')
run_new_parser.set_defaults(func=run_new)
run_new_parser.add_argument('configId', type=str)

def matches_only_one_run(db, regex):
    return matches_only_one(db.run, 'runId', regex)

def run_show(db, args):
    run = matches_only_one_run(db, args.runId)
    del run['_id']
    print(json.dumps(run, sort_keys=True, indent=1))
run_show_parser = run_subparsers.add_parser('show')
run_show_parser.set_defaults(func=run_show)
run_show_parser.add_argument('runId', type=str)

def run_delete(db, args):
    run = matches_only_one_run(db, args.runId)
    runId = run['runId']
    fs = gridfs.GridFS(db)
    fs.delete({'runId':runId})
    db.run.delete_one({'runId' : runId})
    print(runId)
run_delete_parser = run_subparsers.add_parser('delete')
run_delete_parser.set_defaults(func=run_delete)
run_delete_parser.add_argument('runId', type=str)

def run_showfile(db, args):
    fs = gridfs.GridFS(db)
    cursor = fs.find({'runId':{'$regex':args.runId}, 'filename':{'$regex':args.filename}})
    f = next(cursor)
    print(f.read()[:-1])
    pass
run_showfile_parser = run_subparsers.add_parser('showfile')
run_showfile_parser.set_defaults(func=run_showfile)
run_showfile_parser.add_argument('runId', type=str)
run_showfile_parser.add_argument('filename', type=str)

argcomplete.autocomplete(main_parser)
args = main_parser.parse_args()
with pymongo.MongoClient() as client:
    db = client[args.dbname]
    res = args.func(db, args)
    if res != None: print(res)
